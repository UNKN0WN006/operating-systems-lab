# A2_06_03 — Architecture, Memory Layout & Flowchart

This document explains architecture decisions, data movement, memory considerations and includes a Mermaid flowchart (plus ASCII fallback) for `A2_06_03,py.py` (Parallel matrix multiplication using Python multiprocessing).

## Architecture overview

- Language: Python 3 with NumPy and multiprocessing.
- Components:
  - Parent process: constructs matrices A and B, spawns worker processes, collects partial results, assembles final `result` matrix and prints timing.
  - Worker processes: compute `partial = np.dot(A[start:end], B)` for their assigned row-range and send result back via `multiprocessing.Queue`.
- IPC / data movement:
  - On Unix with `fork`, child processes inherit the parent's memory via copy-on-write (no immediate copy) but returning large arrays still requires pickling through the `Queue`.
  - On platforms using `spawn`, A and B will be pickled and sent to child processes at process start.

## Data shapes & memory layout

- A, B: (N, N) `np.uint32` arrays. Size (bytes) ≈ N*N*4 each.
- Partial results: (rows_per_process, N) arrays placed into the result matrix.
- Result: (N, N) `np.uint32` array assembled in parent.

## Flowchart (Mermaid)

```mermaid
flowchart TD
  P[Parent] -->|Create A,B| P
  P -->|Compute step & spawn| W1[Worker 1]
  P --> W2[Worker 2]
  P --> Wn[Worker n]
  W1 -->|partial (start, partial)| Q[Queue]
  W2 --> Q
  Wn --> Q
  Q -->|get| P
  P -->|assemble partials| R[Result]
  R -->|print time| Out[Output]
```

## ASCII fallback flow

Parent: create A,B -> compute chunk size -> spawn N processes -> workers compute partial np.dot -> workers put (start_idx, partial) into Queue -> parent reads partials and assembles -> join workers -> print time

## Concurrency & resource considerations

- Start method: `fork` avoids pickling of large arrays at spawn on Unix but may lead to subtle copy-on-write costs if any process touches parent memory. On non-Unix systems use shared memory APIs to avoid pickling.
- BLAS threading: NumPy may call into multi-threaded BLAS. Combining process-level parallelism with BLAS multi-threading may oversubscribe CPUs. Mitigate by setting environment variables (e.g., `OMP_NUM_THREADS=1`) or using single-threaded BLAS.
- IPC overhead: `Queue` pickles the `partial` arrays and transmits them. For large partials this is expensive. Use `multiprocessing.shared_memory` or `numpy.memmap` to avoid copy.

## Correctness & edge cases

- Overflow: Using `np.uint32` can overflow for large sums; prefer `np.uint64` for safety when `mod` is large.
- Zero-row children: If `num_processes > N`, some children may process zero rows. Avoid by capping `num_processes = min(num_procs, N)`.
- Ordering: Parent uses `start_idx` returned by workers to place partial correctly. Ensure uniqueness of `start_idx` and no overlapping ranges.

## Maintainability & improvements

- Replace `Process` + `Queue` with a `Pool` and `starmap` or `map` for simpler lifecycle management.
- Use `multiprocessing.shared_memory` for zero-copy sharing of `A` and `B`.
- Use `np.dot` single-process (multi-threaded BLAS) for simplicity and best performance on NUMA systems, unless distributed memory is required.
- Consider chunking columns instead of rows for better cache locality depending on BLAS layout.

## Small code example (shared-memory idea)

- Use `multiprocessing.shared_memory.SharedMemory` to create memory segments for `A` and `B` and have workers create `np.ndarray` views into them. This avoids pickling and reduces memory usage.

---

If you'd like, I can now:
- Patch `A2_06_03,py.py` to demonstrate a `multiprocessing.shared_memory` version (zero-copy), or add a `Makefile` / `run_small.sh` to run small tests.

Say "apply shared-memory patch" or "create test harness" to continue.