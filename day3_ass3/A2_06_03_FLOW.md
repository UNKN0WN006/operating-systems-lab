# A2_06_03 — Program Flow, Behaviour & Viva Questions

This document describes the program flow, expected behaviour, verification notes and viva questions for `A2_06_03,py.py` (Assignment 3 — Parallel Programming in Python using multiprocessing and NumPy).

## Quick summary

- Purpose: Multiply two N×N matrices in parallel using Python's `multiprocessing` by splitting rows across multiple processes. Measure elapsed time for multiplication and optionally print matrices.
- Invocation:

```bash
python A2_06_03.py <matrix_size> <num_processes> <mod> <print_switch>
# e.g. python A2_06_03.py 3000 4 100 0
```

- Important notes: The code uses `multiprocessing.Process`, passes `A` and `B` as arguments and collects partial results via a `multiprocessing.Queue`.

## Inputs & outputs

- Inputs:
  - `matrix_size` (N): dimension of square matrices A and B.
  - `num_processes`: number of worker processes.
  - `mod`: upper bound for random values (0..mod-1).
  - `print_switch`: 1 to print A, B, and result; 0 to suppress printing.

- Outputs:
  - Console prints showing elapsed time for multiplication and optionally the matrices.
  - Example summary:

```
Matrix multiplication complete using 4 processes.
Time taken (multiplication only): 20.57 seconds
```

## High-level program flow

1. Parse and validate command-line arguments. Exit with usage message if bad args.
2. Create random matrices `A` and `B` as NumPy arrays of shape (N, N) and dtype `np.uint32`.
3. Optionally print `A` and `B` if `print_switch` is true (not recommended for large N).
4. Compute `step = N // num_processes` (rows per process). For each process `i`:
   - Compute `start = i * step`, `end = (i+1)*step` (last process takes remainder).
   - Spawn `mp.Process(target=multiply_rows, args=(start, end, A, B, result_queue))`.
   - Each worker computes `partial = np.dot(A[start:end], B)` and `result_queue.put((start, partial))`.
5. Parent creates a zero matrix `result` and collects `num_procs` partial results from `result_queue`.
6. Parent assigns each partial into `result[start_idx:start_idx+rows]`.
7. Parent `join()`s all processes and prints elapsed time and optionally the `result` matrix.

## Data movement and performance implications

- Passing `A` and `B` to `Process`:
  - On Unix-like systems using the `fork` start method, child processes inherit the parent address space (copy-on-write), so large arrays are not duplicated immediately. However, operations that modify arrays in child processes will cause page-copying.
  - On platforms using the `spawn` start method (Windows, or if explicitly set), passing `A` and `B` will cause them to be pickled and sent to child processes — a heavy cost for large matrices.
- Returning partial results via `multiprocessing.Queue` also pickles NumPy arrays; partial results may be large and impose IPC overhead and memory pressure.
- For best performance on large matrices on all platforms consider:
  - Using `multiprocessing.shared_memory` to share `A` and `B` without pickling.
  - Using `numpy.memmap` to memory-map shared backing storage.
  - Using a `Pool` of workers with `map`/`imap` to manage workers more ergonomically.

## Correctness & edge cases

- Integer overflow: `np.uint32` can overflow when multiplying large values; if exact results are required, use a larger dtype (e.g., `np.uint64` or Python `object`/`int`).
- When `num_processes > N`: some processes will receive zero rows. Code handles last process extending to `N` but still creates processes that may do zero work — consider limiting `num_processes` to `min(num_processes, N)`.
- Ordering: `result_queue.get()` returns partial results in arbitrary order; the code uses the `start_idx` returned with partial to place each chunk correctly into `result`.

## Testing checklist

- Small correctness test:
  - `python A2_06_03.py 4 2 10 1` and verify result with a serial `np.dot(A, B)` computed in-process.
- Performance tests:
  - Run with different `num_processes` (1, 2, 4, 8) and record elapsed times to observe scaling.
  - Monitor CPU with `mpstat -P ALL 1` or `top` to confirm multiple cores are utilized.
- Memory tests:
  - Try large N (e.g., 3000) and watch memory usage; ensure system has adequate RAM or kernel will swap.
- Cross-platform tests:
  - On Windows, `multiprocessing` uses `spawn` by default; test behavior and time costs (pickling cost) and adapt to `multiprocessing.shared_memory` if needed.

## Observability: CPU snapshots (example from sample header)

- The included sample shows `mpstat` output with some CPU cores near 100% while others idle, indicating work distribution across cores.

## Suggestions for improvements

- Use `multiprocessing.shared_memory` (Python 3.8+) to share `A` and `B` without pickling.
- Use `sharedctypes` or `posix_ipc` with NumPy `ndarray` views if portability to older Python versions is required.
- Use `Pool.map` or `concurrent.futures.ProcessPoolExecutor` to simplify worker lifecycle and error handling.
- Return results via shared-memory buffers and signal completion via small control messages to avoid large pickling overhead.
- For very large matrices, consider BLAS-backed libraries (OpenBLAS, MKL) and call NumPy's dot which already parallelizes — sometimes spawning multiple processes is redundant if NumPy is linked to multi-threaded BLAS.

---

## Viva questions (with short model answers)

1. Q: Why use `multiprocessing` instead of `threading` in Python for CPU-bound matrix multiplication?
   - A: Python threads are subject to the Global Interpreter Lock (GIL), which prevents multiple threads from executing Python bytecode simultaneously. Multiprocessing spawns separate processes with independent Python interpreters, enabling parallel CPU-bound execution.

2. Q: How does `np.dot` behave inside child processes — is it multi-threaded itself?
   - A: Many NumPy builds use multi-threaded BLAS (OpenBLAS, MKL). `np.dot` may use internal threads; combining that with multiprocessing can lead to oversubscription. Tune BLAS environment variables (e.g., OMP_NUM_THREADS) or use single-threaded BLAS for each process.

3. Q: What is the cost of sending `A` and `B` to child processes when using the default `spawn` method?
   - A: They are pickled and sent over a pipe, which serializes and copies the entire arrays — high overhead for large arrays. On Unix with `fork` this cost is avoided at process start (copy-on-write), but portability concerns remain.

4. Q: Why does the code use `result_queue.put((start, partial))`? Are there alternatives?
   - A: The queue transmits partial results along with their start row index so the parent can place them correctly. Alternatives: write partial results into a pre-allocated shared-memory result buffer or use file-backed memory maps.

5. Q: How do you avoid pickling overhead for large NumPy arrays in multiprocessing?
   - A: Use `multiprocessing.shared_memory` (Python 3.8+), `numpy.memmap`, or use `fork` start method (Unix) while ensuring children don't write to inherited arrays (to preserve COW). Also use `sharedctypes` with `ndarray` views.

6. Q: How would you debug incorrect results or shape mismatches?
   - A: Add assertions checking shapes, compare the result to `np.dot(A, B)` computed serially for small N, and log `start`/`end` ranges returned by workers.

7. Q: If `np.dot` itself is multi-threaded, what happens when you also fork multiple worker processes?
   - A: You can get thread oversubscription; each process may spawn multiple BLAS threads, causing many threads to contend for CPU. Limit BLAS threads per process (e.g., `OMP_NUM_THREADS=1`) or use BLAS that supports internal throttling.

8. Q: Why does the code set `result` dtype to `np.uint32`? Any issues?
   - A: It matches the input dtype and saves memory, but `uint32` can overflow during multiplication/summation. If correctness for large/mod values is required, use a larger dtype like `uint64` or `int64`.

9. Q: How would you measure scalability and what metrics would you capture?
   - A: Capture elapsed wall-clock time, CPU utilization across cores, memory usage, and IPC overhead (time spent pickling/unpickling). Use `time`, `mpstat`, `top`, and profilers.

10. Q: What are safer deployment choices for production HPC-style matrix multiplication?
    - A: Use optimized BLAS/LAPACK libraries (OpenBLAS, Intel MKL), call multi-threaded `numpy.dot` directly (possibly single process), or use MPI-based distributed matrix multiply for very large problems.

---

If you want, I can now:
- Create `day3_ass3/A2_06_03_ARCH.md` with the architecture, memory layout, Mermaid flowchart and maintainers' notes (I will create it next).  
- Patch the Python to use `multiprocessing.shared_memory` for zero-copy sharing and include a small test harness.  

Say "create architecture doc" or "apply shared-memory patch" to proceed.