# A2_06_06 — Program Flow, Threading & Verification

This document explains the flow and behaviour of `A2_06_06.c` (Parallel matrix multiplication using POSIX threads). It includes the program flow, mapping to assignment requirements, verification notes, sample outputs and viva questions.

## Overview

Objective recap:
- Multiply two N×N matrices A and B to produce C = A × B.
- Use multiple POSIX threads; each thread computes a subset of rows of C.
- Allow control via command-line: matrix size N, number of threads T, modulo for random init, and print switch.
- Measure and print elapsed time.

Build & run

```bash
gcc A2_06_06.c -o A2_06_06 -lpthread -lm
./A2_06_06 <matrix_size> <num_threads> <mod_value> <print_switch>
# Example: ./A2_06_06 1000 4 100 0
```

---

## High-level program flow

1. `main(argc, argv)`
   - Validate 4 command-line arguments: N, num_threads, mod_value, print_switch.
   - Parse arguments, print a short header showing the configuration.

2. Memory allocation & initialization
   - Allocate three matrices A, B, C as `unsigned char**` (rows of byte arrays).
   - Initialize A and B with random values `rand() % mod_value`.
   - If `print_switch == 1`, print A and B (compact format if N>10).

3. Timing start
   - Record timestamp using `gettimeofday(&start_time)`.

4. Create worker threads
   - Compute `rows_per_thread = N / T`, `remaining_rows = N % T`.
   - For each thread i (0..T-1), set `start_row` and `end_row` in `thread_data`.
   - `pthread_create` thread to run `multiply()` with its `thread_data`.

5. Worker function `multiply(void* arg)`
   - For each assigned row i and for each column j, compute dot product sum over k: sum += A[i][k] * B[k][j].
   - Store result in C[i][j] as `sum % 256` (fits in `unsigned char`).

6. Join threads
   - `pthread_join` all worker threads.

7. Timing end and reporting
   - Record `gettimeofday(&end_time)` and compute elapsed time in seconds.
   - Print elapsed time.
   - Optionally print result matrix C if `print_switch == 1`.

8. Cleanup
   - Free matrices and thread data.

---

## Mapping to requirements / correctness checks

- Parallelization: Each thread computes disjoint row ranges — no synchronization needed.
- Work division: rows split evenly with the last thread taking the remainder.
- Random initialization: `rand() % mod_value` used; `srand(time(NULL))` called during init (note: called twice in current code: in `init_matrix` which may reseed each call; acceptable but could be seeded once in main).
- Timing: `gettimeofday` used to time the critical region (creation to join of worker threads) and `get_time_diff` returns elapsed seconds.
- Result correctness: For small N/testcases, compare serial multiplication result with threaded result to validate correctness (optional test to add).

---

## Performance & expected behaviour

- CPU utilization: Running with multiple threads will utilize multiple cores; use `sar`, `top`, or `htop` to observe CPU usage.
- Memory: Using `unsigned char` for matrices reduces memory pressure; N=3000 yields ~9 MB per matrix (3000*3000 bytes ~ 9,000,000 bytes) — three matrices ~27 MB.
- Time scales approximately as O(N^3)/T (assuming uniform distribution and good cache behaviour).

---

## Verification & testing checklist

1. Small correctness test (N small, print_switch=1):
   - Run `./A2_06_06 4 2 10 1` and manually verify C = A×B or compare to a serial implementation.

2. Performance test (large N):
   - Run with different numbers of threads (1, 2, 4, 8) and record elapsed times; expect decreasing time with more threads up to CPU core count.
   - Use `sar -u 1 60` or `top` to capture CPU utilization.

3. Memory check:
   - Ensure allocation succeeds for the required N; program uses `malloc` for each row. Check for `NULL` returns if N too large.

4. Edge cases:
   - N less than T (threads > rows): some threads get zero rows — code handles this by division, but verify no invalid ranges.
   - mod_value <= 0: currently allows 0 causing modulo by 0 — add guard or require mod_value >= 1.
   - Very large N may cause `malloc` to fail; handle errors gracefully.

---

## Suggestions / small fixes

- Seed RNG once in `main` (call `srand(time(NULL))` once), not inside `init_matrix` (which is called twice) to prevent correlated randoms.
- Check return of `malloc` in `allocate_matrix` for each row and abort with `perror` if `NULL`.
- Consider using contiguous 1-D arrays for better cache locality (single malloc of size N*N) to speed up large multiplications.
- Optionally add a serial multiplication mode to validate threaded output.

---

## Sample output (from repo header)

```
Matrix size: 3000x3000, Threads: 4, Mod: 100
Time elapsed: 96.886 seconds
```

For small N with print switch 1, program prints compact matrix previews for A, B, and full/sampled C.

---

## Important viva questions (with short model answers)

1. Q: How did you parallelize matrix multiplication? Why row-wise division?
   - A: Each thread computes a disjoint set of rows of C. Row-wise division avoids synchronization because each row is written by one thread only.

2. Q: How do you ensure threads don't write to the same memory concurrently?
   - A: Work is partitioned by rows; each element C[i][j] is written only by the thread assigned row i, so no locking required.

3. Q: Why use `unsigned char` for matrix storage? Pros and cons?
   - A: Saves memory and improves cache locality. Con: values are limited to 0–255; we apply `sum % 256` to fit results; for accurate numeric results use larger types.

4. Q: How is work divided when N is not divisible by T?
   - A: `rows_per_thread = N / T`, remainder assigned to the last thread by extending its `end_row` by `remaining_rows`.

5. Q: How would you optimize this code for high performance?
   - A: Use contiguous arrays, block (tiling) multiplication to exploit cache, use SIMD (OpenMP, BLAS), reduce function-call overhead, prefetching.

6. Q: Why does parallelization not always scale linearly with threads?
   - A: Overheads (thread scheduling, memory bandwidth, cache contention), limited cores, and Amdahl's law.

7. Q: How do you measure elapsed time and why `gettimeofday`?
   - A: `gettimeofday` gives wall-clock time; it's portable. For higher precision use `clock_gettime(CLOCK_MONOTONIC, ...)`.

8. Q: Where could race conditions appear in this program?
   - A: If two threads were assigned overlapping rows or the allocation/initialization routines were shared unsafely. Current design avoids races for C writes.

9. Q: How would you test correctness for large matrices efficiently?
   - A: Use known small matrices testcases, compute checksum/hash of result (e.g., sum of all elements modulo a value) and compare against serial or reference implementation.

10. Q: What about memory fragmentation or allocation overhead for per-row mallocs?
    - A: Per-row mallocs cost more overhead; allocating one contiguous block for the entire matrix reduces fragmentation and improves performance.

---

If you want, I can now:
- Create `A2_06_06_ARCH.md` with the architecture, memory layout, and a Mermaid flowchart (I will add an ASCII fallback). 
- Make small code fixes: seed RNG once, check `malloc` returns, and improve robustness (I can implement these edits and run quick sanity checks).

Which would you like next?